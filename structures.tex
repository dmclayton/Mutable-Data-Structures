In general, each probabilistic data structure has some bound on the error size per query, assuming the adversary is not fully adaptive. In each case we want to show that allowing the adversary full adaptivity does not significantly increase the error rate. Generally, but not always, we have a response space $\mathcal{R}$ and a data object space $\mathcal{D} \subseteq Func(\mathcal{X},\mathcal{R})$ for some universe $\mathcal{X}$, so that each object in the universe is associated with a single correct response. The usual query space consists of indicator functions $\qry_x$ for $x \in \mathcal{X}$, so that $\qry_x(\col) = \col(x)$ for all $\col \in \mathcal{D}$. The update space at least consists of insertions to add a single element, but may also include deletions of single elements.

In each of the following cases, we consider four variations, where each structure may be constructed with or without a randomly-chosen per-representation salt and with or without a private key. We also sometimes consider the possibility of adding a `threshold' to detect when a structure is too full. For example, we may modify the $\Up$ algorithm of a counting filter to fail if it is called to insert an element when the sum over all the counters in the filter has reached some threshold value. This variation can in some cases provide substantially better error bounds by preventing the adversary from exploiting more naive limits on the fullness of a filter, such as the number of elements in the set being represented.

\begin{table}[t]
\begin{center}
\small
  \begin{tabular}{ | l | p{2.5cm} | p{3.0cm} | p{4cm} | p{3cm}|}
    \hline
    {\bf Structure} & {\bf Data Objects} & {\bf Supported Queries} & {\bf Supported Updates} & {\bf Parameters} \\ \hline
    Bloom filter 
          & \parbox[c][6ex]{4cm}{Sets\\$\col\subseteq \mathcal{X}$} %, or\\ $\col \in \Func(\mathcal{X},\{0,1\})$} 
          & $\qry_x(\col) = [x \in \col]$
          &  $\up_x(\col) = \col \cup \{x\}$ 
          & \parbox[c]{4cm}{$n$, max $|\col|$\\$k$, no.\ hash functions\\$m$, array size (bits)} 
          \\\hline
    Counting filter 
          & \parbox[c]{3.5cm}{Multisets\\ $\col \in \Func(\mathcal{X},\N)$}
          & $\qry_x(\col) = [\col(x) > 0]$
          & \parbox[c][10ex]{4cm}{$\up_{x,0}(\col)(x) = \col(x)+1$ \\ $\up_{x,1}(\col)(x) = \col(x)-1$ \\ $\up_{x,b}(\col)(y) = \col(y)$ for $x \neq y$}
          & \parbox[c]{3cm}{\dctodo{fill in}}
         \\ \hline
    Cuckoo filter
          & \parbox[c]{3.5cm}{Multisets\\ $\col \in \Func(\mathcal{X},\N)$}
          & $\qry_x(\col) = [\col(x) > 0]$
          & \parbox[c][10ex]{4cm}{$\up_{x,0}(\col)(x) = \col(x)+1$ \\ $\up_{x,1}(\col)(x) = \col(x)-1$ \\ $\up_{x,b}(\col)(y) = \col(y)$ for $x \neq y$}
          & \parbox[c]{3cm}{\dctodo{fill in}}
          \\ \hline
     Count-min sketch
          & \parbox[c]{3.5cm}{Multisets\\ $\col \in \Func(\mathcal{X},\N)$}
          & $\qry_x(\col) = \col(x)$
          & \parbox[c][10ex]{4cm}{$\up_{x,0}(\col)(x) = \col(x)+1$ \\ $\up_{x,1}(\col)(x) = \col(x)-1$ \\ $\up_{x,b}(\col)(y) = \col(y)$ for $x \neq y$}
          & \parbox[c]{3cm}{\dctodo{fill in}}
          \\ \hline
   
  \end{tabular}
\end{center}
\caption{The data structrues that we consider.  The set~$\mathcal{X}$ is some understood universe of base objects.  Each data structure yields a space-efficient representation of its input data object and, in the presense of non-adaptive attacks, provides approximately correct responses to the supported queries.  For counting filters, cuckoo filters, and count-min sketch, typical implementations prevent updates that would cause $\col(x)-1 < 0$.}
\end{table}

\ignore{
\begin{table}[thp]
\begin{center}
  \begin{tabular}{ | c | p{4cm} | p{4cm} | p{4cm} | p{4cm} | }
    \hline
    Structure & $\mathcal{D}$ & $\mathcal{R}$ & $\mathcal{Q}$ & $\mathcal{U}$ \\ \hline
    Bloom filter & $[\mathcal{X}]^{\le n}$ for some universe $\mathcal{X}$ & $\bits$ & $\{\qry_x: x \in \mathcal{X}\}$, where $\qry_x(\col) = [x \in \col]$ & $\{\up_x : x \in \mathcal{X}\}$, where $\up_x(\col) = \col \cup \{x\}$ \\ \hline
    Counting filter & $Func(\mathcal{X},\Z)$ & $\bits$ & $\{\qry_x: x \in \mathcal{X}\}$, where $\qry_x(\col) = [\col(x) > 0]$ & $\{\up_{x,b} : x \in \mathcal{X}, b \in \bits\}$, where $\up_{x,0}(\col)(x) = \col(x)+1$, $\up_{x,1}(\col)(x) = \col(x)-1$, and $\up_{x,b}(\col)(y) = \col(y)$ for $x \neq y$ \\ \hline
    Cuckoo filter & $Func(\mathcal{X},\N)$ & $\bits$ & $\{\qry_x: x \in \mathcal{X}\}$, where $\qry_x(\col) = [\col(x) > 0]$ & $\{\up_{x,b} : x \in \mathcal{X}, b \in \bits\}$, where $\up_{x,0}(\col)(x) = \col(x)+1$, $\up_{x,1}(\col)(x) = \col(x)-1$, and $\up_{x,b}(\col)(y) = \col(y)$ for $x \neq y$ \\ \hline
    Count-min sketch & $Func(\mathcal{X},\N)$ & $\Z$ & $\{\qry_x: x \in \mathcal{X}\}$, where $\qry_x(\col) = \col(x)$ & $\{\up_{x,b} : x \in \mathcal{X}, b \in \bits\}$, where $\up_{x,0}(\col)(x) = \col(x)+1$, $\up_{x,1}(\col)(x) = \col(x)-1$, and $\up_{x,b}(\col)(y) = \col(y)$ for $x \neq y$ \\ \hline
  \end{tabular}
\end{center}
\end{table}

\begin{table}[thp]
\begin{center}
  \begin{tabular}{ | c | p{4cm} | p{4cm} | p{4cm} | }
    \hline
    Structure & $\Rep_K(\col)$ & $\Qry_K(\pub,\qry)$ & $\Up_K(\pub,\up)$ \\ \hline
    Bloom filter & Initialize an array of $m$ zero bits. Let $H_K(x): \mathcal{X} \to [m]^k$ be given by $H_K(x) = (h_1(K \Vert x),\ldots,h_k(K \Vert x))$ for distinct hash functions $h_i$. For each $x \in \col$, and each index $i \in H_K(x)$, set the $i$th bit of the array to 1. & If each bit in the array with an index in $H_K(x)$ is 1, return 1; else return 0. & Fail if the filter is full, else set each bit in the array with an index in $H_K(x)$ to 1. \\ \hline
    Counting filter & Initialize an array of $m$ counters. Let $H_K(x): \mathcal{X} \to [m]^k$ be given by $H_K(x) = (h_1(K \Vert x),\ldots,h_k(K \Vert x))$ for distinct hash functions $h_i$. For each $x \in \col$, and each index $i \in H_K(x)$, increment the $i$th counter of the array. & If each counter in the array with an index in $H_K(x)$ is greater than 0, return 1; else return 0. & Fail if the filter is full, else increment (if $b = 1$) or decrement (if $b = 0$) each counter in the array with an index in $H_K(x)$. \\ \hline
    %Cuckoo filter & Initialize an array of $m$ counters. Let $H_K(x): \mathcal{X} \to [m]^k$ be given by $H_K(x) = (h_1(K \Vert x),\ldots,h_k(K \Vert x))$ for distinct hash functions $h_i$. For each $x \in \col$, and each index $i \in H_K(x)$, increment the $i$th counter of the array. & If each counter in the array with an index in $H_K(x)$ is greater than 0, return 1; else return 0. & Increment (if $b = 1$) or decrement (if $b = 0$) each counter in the array with an index in $H_K(x)$. \\ \hline
    Count-min sketch & Initialize a two-dimensional $k$-by-$m$ array $arr$ of counters. Let $h_1,\ldots,h_k$ be distinct hash functions. For each $x \in \col$, and each $i \in [k]$, increment the $h_i(K \Vert x)$th counter of the $i$th row of the array. & Return $\min_{i \leq k} arr[i][h_i(K \Vert x)]$. & Fail if the sketch is full, else for each $i \in [k]$, increment (if $b = 1$) or decrement (if $b = 0$) the $h_i(K \Vert x)$th counter of the $i$th row of the array. \\ \hline
  \end{tabular}
\end{center}
\end{table}
}
%A typical case occurs with standard Bloom filters~\cite{bloomfilter}, which is parametrized by positive integers $k$, $m$, $n$, representing the number of hash functions used, the size of the filter, and the maximum set size supported by the filter.

%The object space is given by $\mathcal{D} = [\mathcal{X}]^{\le n}$, consisting of all subsets of some universe $\mathcal{X}$ that have cardinality no greater than a constant $n$. The standard $\Rep$ algorithm initializes an array of $m$ bits to zero and then hashes each element of the set using $k$ distinct hash functions $h_i: \mathcal{X} \to [m]$. Each of the indices specified by each of the hash function outputs is then set to 1.

A typical case occurs with standard Bloom filters~\cite{bloomfilter}. In this case we have a response space $\mathcal{R} = \bits$ and an object space $\mathcal{D} = [\mathcal{X}]^{\le n}$ consisting of all subsets of some universe $\mathcal{X}$ that have cardinality no greater than a constant $n$. The queries are standard membership queries for each element of $\mathcal{X}$, and the update space consists of insertions. An update to insert the element $x$ is denoted by $\up_x$. These data structures are constructed using a set of hash functions, each of which maps an object to a position in an array of bits. When an object is added to the filter, either during the filter's creation or during a later update, each of the bits the object is mapped to by each of the hash functions is set to 1. To make a membership query, one can simply hash the queried object and check whether all of the associated bits are 1. Updates may only add more elements to the set: with a traditional Bloom filter structure, deletion is impossible. Because of this, Bloom filters have no false negatives, and the size of the error a non-adaptive adversary is expected to create per query is simply equal to the false positive rate, which is on the order of $(1-e^{-\frac{kn}{m}})^k$ for an $m$-bit array with $k$ hash functions storing up to $n$ values.

%Compressed Bloom filters~\cite{xxx} operate in the same way, with a false positive rate which must also take into account the degree of compression. The maximum amount we can compress a Bloom filter is determined by the probability that a given bit in the filter is 0, which is $\left(1-\frac{1}{m}\right)^{kn}$. This is closely approximated by $p = e^{-\frac{kn}{m}}$. For a given $p$, an optimal compressor will reduce an $m$-bit filter to $mH(p)$ bits, where $H$ is the entropy function $H(p) = -p\log p - (1-p)\log(1-p)$. Because of this, in order to compress the original $m$-bit filter down to $z$ bits we must have $z \ge mH(p)$. Note that an ordinary Bloom filter has false positive rate $(1-p)^k$; in the compressed case we instead have a false positive rate of $(1-p)^{-\frac{z \ln p}{nH(p)}}$.

Counting bloom filters~\cite{xxx}, or counting filters, allow deletion by broadening the data object space to $\mathcal{D} \subseteq Func(\mathcal{X},\Z)$. With insertion and deletion both possible, we denote an insertion update for $x$ by $\up_{x,1}$ and a deletion update for $x$ by $\up_{x,0}$. Under ideal conditions the range of the data objects is not just in $\Z$ but in $\N$ specifically, but negative multiplicities may be introduced by the deletion of objects which are not actually in the set. The queries for a counting Bloom filter are still binary, operating by projecting the underlying multiset down to an ordinary set which contains a given element if and only if the multiset contains that element with multiplicity at least 1. The introduction of a deletion operation means that false positives and false negatives are both possible.% Depending on the application, either of these may be worse than the other, and an error function $d$ must be chosen accordingly.

Cuckoo filters~\cite{xxx} are use the same extension of the data object space and update space, still allowing for only set membership queries along with insertion and deletion updates. Though the structure itself is different, the syntax is the same as for counting filters..

Depending on the implementation, count-min sketch~\cite{xxx} may similarly use a data object space $\mathcal{D} \subseteq Func(\mathcal{X},\Z)$ or may use a smaller space $\mathcal{D} \subseteq Func(\mathcal{X},\N)$. In the former case we have two further sub-cases, the case where all updates increment the value associated with $x \in X$ and the case where updates may either increment or decrement the associated value (but not below 0). Additionally, count-min sketch supports multiple different types of queries, in each case yielding a response in $\mathbb{Z}$. For a point query, the difference between the query and the true value is bounded by $n\epsilon$ with probability $1-\delta$, where $n$ is the sum of the true frequencies of the stream elements. The maximum error of a single query is simply $n$, in the case that an element has never actually been added to the set but has incorrectly had all its counters incremented each of $n$ times another element has been added, so the expected non-adaptive error size is bounded above by $n\epsilon(1-\delta)+n\delta = n(\delta+\epsilon-\delta\epsilon)$.  Similarly, we find that the error size of an inner product query is bounded above by $n_1n_2\epsilon(1-\delta)+n_1n_2\delta = n_1n_2(\delta+\epsilon-\delta\epsilon)$. Finally, range queries [...]

%Bloomier filters~\cite{xxx} are designed to represent arbitrary functions instead of set membership or multiplicity, providing a more general response space $\mathcal{R}$ than the binary $\{0,1\}$ used with Bloom filters and altering the data object space to a set of the form $\mathcal{D} \subseteq Func(\mathcal{X},\mathcal{R})$. One of the response values is $\bot$ to indicate that value has not been associated with any element of $\mathcal{R}$, and the only type of error that can occur is that a value which should return $\bot$ instead returns some other element of $\mathcal{R}$. Again we have a situation where `false positives' are the only type of error which can occur. Update queries allow for changes in the element of $\mathcal{R}$ associated with each element of $\mathcal{X}$, though changing an element's associated value to or from $\bot$ is not permitted: neither insertion nor deletion is possible with this structure.

Stable Bloom filters~\cite{xxx} are an example of a structure in the Bloom filter family which our notions cannot work with easily. One way in which they are unique is in featuring a probabilistic update algorithm, causing objects in the filter to probabilistically decay over time. While our syntax accounts for that, the more significant difference is that a stable Bloom filter only has good accuracy guarantees once it has seen enough updates to `stabilize'. Our current security notions do not encompass this type of conditional accuracy guarantee.

Many of these structures follow the same design philosophy in order to reduce false positives. Specifically, the value associated with an element (such as a single bit in the case of a set represented by a Bloom filter, or an integer in the case of a multiset represented by count-min sketch) is stored several times. When the data is to be retrieved, we check each of the locations the data has been stored and take the minimal value across all the locations. For example, a Bloom filter returns 0 if any of the hash locations has a 0 bit, while a point query for count-min sketch returns the minimum value across all stored locations. This is as strict an approach as possible when attempting to avoid overestimating the number of times an element has been added to the underlying (multi)set, but can cause problems in an adversarial environment. The tendency to underestimate the number of times an element is represented in the (multi)set means that an adversary can cause errors by decrementing only a single one of the several values associated with the element.

Consider the case of a counting Bloom filter representing a multiset $\col$. When the filter is constructed, each of the $x \in \col$ causes several counters in the filter to be incremented. When a query is made for an element $x'$, we hash $x'$ several times and check whether the value associated with each of the hash values is positive, returning 1 if so and 0 otherwise. If $x' \not\in \col$ but $x'$ does return 1 when queried, we can update the filter to decrement the multiplicity of $x'$, decrementing each of the hash values as well. For every 1 which is decremented to 0 by this operation, a false negative is created: an object which is contained in $\col$ but which will return 0 when queried. This potentially introduces as many false negatives as there are locations associated with an object. In the case of Bloom filters this is an optimal choice, since false negatives are impossible and we can simply optimize for the lowest possible chance of a false positive.

We can consider the possibility of a more symmetric data structure which balances between resistance to false positives and resistance to false negatives. For example, consider a `balanced' counting Bloom filter which answers queries by checking whether the \emph{majority} of associated values are positive, rather than checking whether all associated values are positive. For count-min sketch, we can consider the case of a `count-median-sketch' which returns the mean or median of the associated values rather than the minimum. This variation has been defined previously, but we might also consider arbitrary count-$f$-sketches, where $f: \Z^k \to \Z$ is a function mapping the $k$ counter values to a frequency estimate.

More generally, for any structure of this type with object space $\mathcal{D} \subset Func(\mathcal{U},\mathcal{V})$ for some universe $\mathcal{U}$ of objects with associated values in $\mathcal{V}$, we can take a function $f: \mathcal{V}^* \to \mathcal{R}$ and use it to define $\qry: \mathcal{U} \times Func(\mathcal{U},\mathcal{V}) \to \mathcal{R}$. We evaluate the function by evaluating $f(v_0,\ldots)$ where the $v_i$ are the several values associated with the element being queried. Each of the above structures uses $f = \min$, but many other functions could be used.

An important consideration for probabilistic data structures in the streaming setting is the question of when the structure is considered to be full. For example, a Bloom filter of a fixed size using some number of hash functions can only guarantee good performance up to some maximum number of elements contained. In the streaming setting, we may not know in advance exactly when this threshold is reached. Allowing the insertion of arbitrarily many elements is undesirable because an overfull filter will produce large numbers of false positives. In addition to the straightforward possibility of keeping track of a counter of the number of elements considered in order to not ensure a maximum is surpassed, we consider an alternative where some property of the structure itself is kept under a fixed threshold. For example, with Bloom filters we might require that the total number of bits set to 1 be no more than some proportion $p$ of the total bits, while with counting filters we might do the same for the total number of nonzero counters.