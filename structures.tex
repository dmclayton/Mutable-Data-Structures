In general, each probabilistic data structure has some bound on the error size per query, assuming the adversary is not fully adaptive. In each case we want to show that allowing the adversary full adaptivity does not significantly increase the error rate. Generally, but not always, we have a response space $\mathcal{R}$ and a data object space $\mathcal{D} \subseteq Func(\mathcal{X},\mathcal{R})$ for some universe $\mathcal{X}$, so that each object in the universe is associated with a single correct response. The usual query space consists of indicator functions $\qry_x$ for $x \in \mathcal{X}$, so that $\qry_x(\col) = \col(x)$ for all $\col \in \mathcal{D}$. The update space at least consists of insertions to add a single element, but may also include deletions of single elements.

A typical case occurs with standard Bloom filters~\cite{bloomfilter}. In this case we have a response space $\mathcal{R} = \bits$ and an object space $\mathcal{D} = [\mathcal{X}]^{\le n}$ consisting of all subsets of some universe $\mathcal{X}$ that have cardinality no greater than a constant $n$. The queries are standard membership queries for each element of $\mathcal{X}$, and the update space consists of insertions. An update to insert the element $x$ is denoted by $\up_x$. These data structures are constructed using a set of hash functions, each of which maps an object to a position in an array of bits. When an object is added to the filter, either during the filter's creation or during a later update, each of the bits the object is mapped to by each of the hash functions is set to 1. To make a membership query, one can simply hash the queried object and check whether all of the associated bits are 1. Updates may only add more elements to the set: with a traditional Bloom filter structure, deletion is impossible. Because of this, Bloom filters have no false negatives, and we may assume without loss of generality that the error function is simply $d(1,0) = 1$. The size of the error a non-adaptive adversary is expected to create per query is simply equal to the false positive rate, which is on the order of $(1-e^{-\frac{kn}{m}})^k$ for an $m$-bit array with $k$ hash functions storing up to $n$ values.

Compressed Bloom filters~\cite{xxx} operate in the same way, with a false positive rate which must also take into account the degree of compression. The maximum amount we can compress a Bloom filter is determined by the probability that a given bit in the filter is 0, which is $\left(1-\frac{1}{m}\right)^{kn}$. This is closely approximated by $p = e^{-\frac{kn}{m}}$. For a given $p$, an optimal compressor will reduce an $m$-bit filter to $mH(p)$ bits, where $H$ is the entropy function $H(p) = -p\log p - (1-p)\log(1-p)$. Because of this, in order to compress the original $m$-bit filter down to $z$ bits we must have $z \ge mH(p)$. Note that an ordinary Bloom filter has false positive rate $(1-p)^k$; in the compressed case we instead have a false positive rate of $(1-p)^{-\frac{z \ln p}{nH(p)}}$.

Counting bloom filters~\cite{xxx} allow deletion by broadening the data object space to $\mathcal{D} \subseteq Func(\mathcal{X},\Z)$. With insertion and deletion both possible, we denote an insertion update for $x$ by $\up_{x,1}$ and a deletion update for $x$ by $\up_{x,0}$. Under ideal conditions the range of the data objects is not just in $\Z$ but in $\N$ specifically, but negative multiplicities may be introduced by the deletion of objects which are not actually in the set. The queries for a counting Bloom filter are still binary, operating by projecting the underlying multiset down to an ordinary set which contains a given element if and only if the multiset contains that element with multiplicity at least 1. The introduction of a deletion operation means that false positives and false negatives are both possible. Depending on the application, either of these may be worse than the other, and an error function $d$ must be chosen accordingly.

Cuckoo filters~\cite{xxx} are use the same extension of the data object space and update space, still allowing for only set membership queries along with insertion and deletion updates. Though the structure itself is different, the syntax is the same as for counting filters..

Depending on the implementation, count-min sketch~\cite{xxx} may similarly use a data object space $\mathcal{D} \subseteq Func(\mathcal{X},\Z)$ or may use a smaller space $\mathcal{D} \subseteq Func(\mathcal{X},\N)$. In the former case we have two further sub-cases, the case where all updates increment the value associated with $x \in X$ and the case where updates may either increment or decrement the associated value (but not below 0). Additionally, count-min sketch supports multiple different types of queries, in each case yielding a response in $\mathbb{Z}$. For a point query, the difference between the query and the true value is bounded by $n\epsilon$ with probability $1-\delta$, where $n$ is the sum of the true frequencies of the stream elements. The maximum error of a single query is simply $n$, in the case that an element has never actually been added to the set but has incorrectly had all its counters incremented each of $n$ times another element has been added, so the expected non-adaptive error size is bounded above by $n\epsilon(1-\delta)+n\delta = n(\delta+\epsilon-\delta\epsilon)$.  Similarly, we find that the error size of an inner product query is bounded above by $n_1n_2\epsilon(1-\delta)+n_1n_2\delta = n_1n_2(\delta+\epsilon-\delta\epsilon)$. Finally, range queries [...]

Bloomier filters~\cite{xxx} are designed to represent arbitrary functions instead of set membership or multiplicity, providing a more general response space $\mathcal{R}$ than the binary $\{0,1\}$ used with Bloom filters and altering the data object space to a set of the form $\mathcal{D} \subseteq Func(\mathcal{X},\mathcal{R})$. One of the response values is $\bot$ to indicate that value has not been associated with any element of $\mathcal{R}$, and the only type of error that can occur is that a value which should return $\bot$ instead returns some other element of $\mathcal{R}$. Again we have a situation where `false positives' are the only type of error which can occur. Update queries allow for changes in the element of $\mathcal{R}$ associated with each element of $\mathcal{X}$, though changing an element's associated value to or from $\bot$ is not permitted: neither insertion nor deletion is possible with this structure.

Stable Bloom filters~\cite{xxx} are an example of a structure in the Bloom filter family which our notions cannot work with easily. One way in which they are unique is in featuring a probabilistic update algorithm, causing objects in the filter to probabilistically decay over time. While our syntax accounts for that, the more significant difference is that a stable Bloom filter only has good accuracy guarantees once it has seen enough updates to `stabilize'. Our current security notions do not encompass this type of conditional accuracy guarantee.

Many of these structures follow the same design philosophy in order to reduce false positives. Specifically, the value associated with an element (such as a single bit in the case of a set represented by a Bloom filter, or an integer in the case of a multiset represented by count-min sketch) is stored several times. When the data is to be retrieved, we check each of the locations the data has been stored and take the minimal value across all the locations. For example, a Bloom filter returns 0 if any of the hash locations has a 0 bit, while a point query for count-min sketch returns the minimum value across all stored locations. This is as strict an approach as possible when attempting to avoid overestimating the number of times an element has been added to the underlying (multi)set, but can cause problems in an adversarial environment. The tendency to underestimate the number of times an element is represented in the (multi)set means that an adversary can cause errors by decrementing only a single one of the several values associated with the element.

Consider the case of a counting Bloom filter representing a multiset $\col$. When the filter is constructed, each of the $x \in \col$ causes several counters in the filter to be incremented. When a query is made for an element $x'$, we hash $x'$ several times and check whether the value associated with each of the hash values is positive, returning 1 if so and 0 otherwise. If $x' \not\in \col$ but $x'$ does return 1 when queried, we can update the filter to decrement the multiplicity of $x'$, decrementing each of the hash values as well. For every 1 which is decremented to 0 by this operation, a false negative is created: an object which is contained in $\col$ but which will return 0 when queried. This potentially introduces as many false negatives as there are locations associated with an object. In the case of Bloom filters this is an optimal choice, since false negatives are impossible and we can simply optimize for the lowest possible chance of a false positive.

We can consider the possibility of a more symmetric data structure which balances between resistance to false positives and resistance to false negatives. For example, consider a `balanced' counting Bloom filter which answers queries by checking whether the \emph{majority} of associated values are positive, rather than checking whether all associated values are positive. For count-min sketch, we can consider the case of a `count-average sketch' or `count-median-sketch' which returns the mean or median of the associated values rather than the minimum.

More generally, for any structure of this type with object space $\mathcal{D} \subset Func(\mathcal{U},\mathcal{V})$ for some universe $\mathcal{U}$ of objects with associated values in $\mathcal{V}$, we can take a function $f: \mathcal{V}^* \to \mathcal{R}$ and use it to define $\qry: \mathcal{U} \times Func(\mathcal{U},\mathcal{V}) \to \mathcal{R}$. We evaluate the function by evaluating $f(v_0,\ldots)$ where the $v_i$ are the several values associated with the element being queried. Each of the above structures uses $f = \min$, but many other functions could be used.